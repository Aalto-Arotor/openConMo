{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to utilize the downloaded datasets with Python\n",
    "\n",
    "First, run the download functions in order to process the data into the required format.\n",
    "\n",
    "### Notes\n",
    "\n",
    "Make sure you have enough working memory if you are injecting a large amount of data into a single dataframe. Furthermore, .ipynb kernels terminate rather easily, so larger amounts of data should be processed in .py environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paderborn\n",
    "\n",
    "\n",
    "All bearing codes = [\n",
    "        \"K001\",\n",
    "        \"K002\",\n",
    "        \"K003\",\n",
    "        \"K004\",\n",
    "        \"K005\",\n",
    "        \"K006\",\n",
    "        \"KA01\",\n",
    "        \"KA03\",\n",
    "        \"KA04\",\n",
    "        \"KA05\",\n",
    "        \"KA06\",\n",
    "        \"KA07\",\n",
    "        \"KA08\",\n",
    "        \"KA09\",\n",
    "        \"KA15\",\n",
    "        \"KA16\",\n",
    "        \"KA22\",\n",
    "        \"KA30\",\n",
    "        \"KB23\",\n",
    "        \"KB24\",\n",
    "        \"KB27\",\n",
    "        \"KI01\",\n",
    "        \"KI03\",\n",
    "        \"KI04\",\n",
    "        \"KI05\",\n",
    "        \"KI07\",\n",
    "        \"KI08\",\n",
    "        \"KI14\",\n",
    "        \"KI16\",\n",
    "        \"KI17\",\n",
    "        \"KI18\",\n",
    "        \"KI21\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Consult the original publication for precise fault information for each setting and brg_code: https://mb.uni-paderborn.de/fileadmin-mb/kat/PDF/Veroeffentlichungen/20160703_PHME16_CM_bearing.pdf \n",
    "\n",
    "The 'setting' column is essentially the name of the file the data was extracted from. It can be used to differentiate measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data/Paderborn/Paderborn_chunk_1.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_paderborn(args, base_dir=\"data/Paderborn\"):\n",
    "    '''\n",
    "    Searches the four chunk files generated by running paderborn.py twice for all data with the given brg_code and setting.\n",
    "    Parameters:\n",
    "        args (dict): Dictionary where keys are column names and values are lists of accepted values. Supported keys: 'brg_code', 'setting'\n",
    "        base_dir (str): Directory containing Paderborn parquet chunk files.\n",
    "\n",
    "    Returns: \n",
    "        Pandas dataframe with found data\n",
    "\n",
    "    '''\n",
    "    if not args or not isinstance(args, dict):\n",
    "        raise ValueError(\"args must be a non-empty dictionary with column filters.\")\n",
    "    base_path = Path(base_dir)\n",
    "    result_dfs = []\n",
    "    \n",
    "    for chunk_dir in sorted(base_path.glob(\"*_chunk_*\")):\n",
    "        print(f\"Scanning {chunk_dir}\")\n",
    "        try:\n",
    "            df = pd.read_parquet(chunk_dir)\n",
    "            print({col: df[col].unique() for col in df.columns}) \n",
    "            condition = pd.Series([True] * len(df))\n",
    "            \n",
    "            for col, valid_values in args.items():\n",
    "                if col in df.columns:\n",
    "                    condition &= df[col].isin(valid_values)\n",
    "                else:\n",
    "                    print(f\"Warning: Column '{col}' not found in data. Ignoring this filter.\")\n",
    "            \n",
    "            filtered = df[condition]\n",
    "            print(filtered)\n",
    "            if not filtered.empty:\n",
    "                result_dfs.append(filtered)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {chunk_dir}: {e}\")\n",
    "\n",
    "    if result_dfs:\n",
    "        return pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"No data found\")\n",
    "        return pd.DataFrame()\n",
    "filters={ \n",
    "        \"brg_code\":[\"KI05\"],\n",
    "        \"setting\":[\"N15_M01_F10_KI05_20\"]\n",
    "    }\n",
    "\n",
    "df = load_paderborn(filters)\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20298269, 10)\n",
      "       time        force  phase_current_1  phase_current_2        speed  \\\n",
      "0  0.000000  1105.180273         1.200547        -0.732864  1499.793664   \n",
      "1  0.000015  1129.594335         1.296976        -0.811385  1499.793981   \n",
      "2  0.000031  1115.556249         1.128224        -0.843758  1499.794297   \n",
      "3  0.000047  1103.349218         1.234297        -0.998045  1499.794613   \n",
      "4  0.000062  1125.932226         1.215700        -0.675695  1499.794930   \n",
      "\n",
      "   temp_2_bearing_module    torque  vibration_1 brg_code              setting  \n",
      "0              43.389893  0.612224    -0.088501     KI05  N15_M01_F10_KI05_20  \n",
      "1              43.496704  0.591323    -0.094604     KI05  N15_M01_F10_KI05_20  \n",
      "2              43.572998  0.563421    -0.079346     KI05  N15_M01_F10_KI05_20  \n",
      "3              43.597412  0.569828     0.494385     KI05  N15_M01_F10_KI05_20  \n",
      "4              43.545532  0.588976     0.064087     KI05  N15_M01_F10_KI05_20  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLN-EMP\n",
    "\n",
    "m_type: Electric, Vibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Path.glob at 0x7fcdd0a313c0>\n",
      "Scanning data/NLN-EMP/NLN_EMP_chunk_1.parquet\n",
      "{'n_poles': array([4, 2]), 'm_type': array(['Vibration'], dtype=object), 'class': array(['align parallel', 'cavitation suction', 'align combination',\n",
      "       'healthy noise', 'coupling', 'align angular', 'unbalance pump',\n",
      "       'unbalance motor', 'healthy', 'cavitation discharge',\n",
      "       'coupling 2D', 'bent shaft', 'broken rotor bar', 'impeller',\n",
      "       'soft foot', 'bearing pump', 'bearing contaminated', 'new motor',\n",
      "       'bearing bpfo', 'stator short', 'bearing bpfi', 'bearing bsf',\n",
      "       'loose foot pump', 'loose foot motor'], dtype=object), 'channel': array([1, 2, 3, 4, 5])}\n",
      "Scanning data/NLN-EMP/NLN_EMP_chunk_2.parquet\n",
      "{'n_poles': array([2, 4]), 'm_type': array(['Vibration', 'Electric'], dtype=object), 'class': array(['stator short', 'bearing bpfo', 'healthy', 'bearing bpfi',\n",
      "       'broken rotor bar', 'impeller', 'soft foot', 'bearing pump',\n",
      "       'healthy noise', 'bearing contaminated', 'new motor',\n",
      "       'bearing bsf', 'loose foot motor', 'loose foot pump',\n",
      "       'align parallel', 'cavitation suction', 'align combination',\n",
      "       'coupling', 'align angular', 'unbalance pump', 'unbalance motor',\n",
      "       'cavitation discharge', 'coupling 2D', 'bent shaft'], dtype=object), 'channel': array([4, 5, 2, 3, 1, 6])}\n",
      "Scanning data/NLN-EMP/NLN_EMP_chunk_3.parquet\n",
      "{'n_poles': array([2]), 'm_type': array(['Electric'], dtype=object), 'class': array(['healthy noise', 'bearing contaminated', 'bearing bpfi',\n",
      "       'new motor', 'bearing bpfo', 'stator short', 'healthy', 'impeller',\n",
      "       'soft foot', 'bearing pump', 'bearing bsf', 'loose foot motor',\n",
      "       'loose foot pump', 'broken rotor bar'], dtype=object), 'channel': array([6, 4, 5, 1, 2, 3])}\n",
      "      time          0          1         2         3          4   5   6   7  \\\n",
      "0  0.00005 -11.262308 -10.924223 -5.451707  4.317139  10.762424 NaN NaN NaN   \n",
      "1  0.00010 -10.418030 -11.544002 -4.912494  4.265099  11.026911 NaN NaN NaN   \n",
      "2  0.00015 -11.135039 -10.953983 -5.974043  4.436112  10.331392 NaN NaN NaN   \n",
      "3  0.00020 -10.694346 -11.355767 -5.196567  4.448061  11.025425 NaN NaN NaN   \n",
      "4  0.00025 -10.542879 -11.355951 -5.792132  4.529677  10.459790 NaN NaN NaN   \n",
      "\n",
      "    8  ...  58  59  60  61  62  63  64  65  66  67  \n",
      "0 NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "1 NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "2 NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "3 NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "4 NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "\n",
      "[5 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "def load_nln_emp(args: dict, base_dir=\"data/NLN-EMP\"):\n",
    "    '''\n",
    "    Searches the chunk files generated by running nln_emp.py for all data matching given filter conditions.\n",
    "\n",
    "    Parameters:\n",
    "        args (dict): Dictionary where keys are column names and values are lists of accepted values.\n",
    "                     Supported keys: 'm_type', 'n_poles', 'class', 'severity', 'speed', \n",
    "        base_dir (str): Directory containing NLN-EMP parquet chunk files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated dataframe containing the filtered data across all chunk files.\n",
    "    '''\n",
    "    if not args or not isinstance(args, dict):\n",
    "        raise ValueError(\"args must be a non-empty dictionary with column filters.\")\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "    result_dfs = []\n",
    "    print(base_path.glob(\"*_chunk_*\"))\n",
    "    for chunk_file in sorted(base_path.glob(\"*_chunk_*\")):\n",
    "        print(f\"Scanning {chunk_file}\")\n",
    "        try:\n",
    "            df = pd.read_parquet(chunk_file)\n",
    "            df = df.copy()  # avoid SettingWithCopyWarning\n",
    "            for col in df.select_dtypes(include=\"object\").columns:\n",
    "                df[col] = df[col].str.strip()\n",
    "            print({col: df[col].unique() for col in args.keys() if col in df.columns})\n",
    "            #print(df.head(3))\n",
    "            condition = pd.Series([True] * len(df))\n",
    "            \n",
    "            for col, valid_values in args.items():\n",
    "                if col in df.columns:\n",
    "                    condition &= df[col].isin(valid_values)\n",
    "                else:\n",
    "                    print(f\"Warning: Column '{col}' not found in data. Ignoring this filter.\")\n",
    "            \n",
    "            filtered = df[condition]\n",
    "            #print(filtered)\n",
    "            if not filtered.empty:\n",
    "                result_dfs.append(filtered)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {chunk_file}: {e}\")\n",
    "\n",
    "    if result_dfs:\n",
    "        return pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return pd.DataFrame()\n",
    "filters = {\n",
    "    \"n_poles\": [2],\n",
    "    \"m_type\":[\"Electric\"],\n",
    "    \"class\": [\"healthy noise\"],\n",
    "    \"channel\":[1],\n",
    "}\n",
    "\n",
    "df = load_nln_emp(filters)\n",
    "#print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data/ASD/ASD_downloaded.parquet\n",
      "{'class': <StringArray>\n",
      "['1', '3', '2', '6', '7', '5', '4', '0', '9', '8']\n",
      "Length: 10, dtype: string, 'rpm': <StringArray>\n",
      "['1000', '750', '1500', '500', '250', '1250']\n",
      "Length: 6, dtype: string}\n",
      "(451800, 14)\n",
      "       time    enc1_ang    enc2_ang   enc3_ang   enc4_ang     enc5_ang  \\\n",
      "0  0.000000  13961988.0  13961988.0  4653996.0 -4653996.0  1163498.875   \n",
      "1  0.000332  13961990.0  13961990.0  4653996.5 -4653996.5  1163499.125   \n",
      "2  0.000664  13961992.0  13961992.0  4653997.0 -4653997.0  1163499.250   \n",
      "\n",
      "       acc1      acc2      acc3      acc4     Torq1     Torq2 class   rpm  \n",
      "0  1.837953  0.695454  1.873710  1.873710  3.973999  0.942261     1  1000  \n",
      "1 -0.786764 -6.515445 -4.137097 -4.137097  3.989563  0.722656     1  1000  \n",
      "2 -1.084027  0.072936 -0.497809 -0.497809  3.995972  1.012451     1  1000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_ASD(args: dict, base_dir=\"data/ASD\"):\n",
    "    '''\n",
    "    Loads filtered data from ASD_download.parquet file.\n",
    "\n",
    "    Parameters: \n",
    "        args (dict): Dictionary with filter conditions. Supported keys: 'class', 'rpm'.\n",
    "        base_dir (str): Directory containing ASD files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered concatenated data.\n",
    "    '''\n",
    "    if not args or not isinstance(args, dict):\n",
    "        raise ValueError(\"args must be a non-empty dictionary with filter values.\")\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "    result_dfs = []\n",
    "\n",
    "    file = next(base_path.glob(\"*.parquet\"), None)\n",
    "\n",
    "    if file is None:\n",
    "        raise FileNotFoundError(f\"No .parquet file found in {base_dir}\")\n",
    "    print(f\"Scanning {file}\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file)\n",
    "        print({col: df[col].unique() for col in args.keys() if col in df.columns})\n",
    "        \n",
    "        for col in df.select_dtypes(include=\"object\").columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "        condition = pd.Series([True] * len(df))\n",
    "        for col, values in args.items():\n",
    "            if col in df.columns:\n",
    "                condition &= df[col].isin(values)\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in chunk, skipping filter.\")\n",
    "\n",
    "        filtered = df[condition]\n",
    "        if not filtered.empty:\n",
    "            result_dfs.append(filtered)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {file}: {e}\")\n",
    "\n",
    "    if result_dfs:\n",
    "        return pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "filters = {\n",
    "    \"class\": [\"1\"],\n",
    "    \"rpm\": [\"1000\"]\n",
    "}\n",
    "\n",
    "df = load_ASD(filters)\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Path.glob at 0x7f7d19731cf0>\n",
      "Scanning data/AGFD/AGFD_chunk_1.parquet\n",
      "{'class': array(['healthy', 'crack', 'pitting', 'wear', 'micropitting'],\n",
      "      dtype=object), 'rpm': array([1000,  750]), 'healthy_GP': array([9, 3, 2, 1, 5, 4, 6, 7, 0]), 'torque': array([ 6, 11,  1])}\n",
      "Scanning data/AGFD/AGFD_chunk_2.parquet\n",
      "{'class': array(['crack', 'micropitting', 'healthy', 'pitting', 'wear'],\n",
      "      dtype=object), 'rpm': array([ 750, 1500,  500]), 'healthy_GP': array([0, 9, 3, 2, 1, 5, 4, 6, 7]), 'torque': array([ 1,  6, 11])}\n",
      "Scanning data/AGFD/AGFD_chunk_3.parquet\n",
      "{'class': array(['wear', 'micropitting', 'crack', 'healthy', 'pitting'],\n",
      "      dtype=object), 'rpm': array([ 500,  250, 1250]), 'healthy_GP': array([0, 9, 3, 2, 1, 5, 4, 6, 7]), 'torque': array([ 1,  6, 11])}\n",
      "(331326, 18)\n",
      "       time  enc1_ang  enc2_ang  enc3_ang  enc4_ang  enc5_ang       acc1  \\\n",
      "0  0.000000     0.000     0.000     0.000    -0.000     0.000  14.651109   \n",
      "1  0.000332     1.998     2.034     0.648     0.666     0.162  79.047661   \n",
      "2  0.000664     4.014     4.032     1.332     1.314     0.324  26.196007   \n",
      "\n",
      "        acc2       acc3       acc4     Torq1     Torq2    class   rpm  torque  \\\n",
      "0 -38.778648  -6.914052  -5.760087  9.780273  1.317871  healthy  1000       6   \n",
      "1  47.732231   1.118753  -4.851206  9.620667  1.144653  healthy  1000       6   \n",
      "2  36.811794  12.214314  30.963770  9.106750  1.509888  healthy  1000       6   \n",
      "\n",
      "   installation severity  healthy_GP  \n",
      "0             0        -           9  \n",
      "1             0        -           9  \n",
      "2             0        -           9  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_AGFD(args: dict, base_dir=\"data/AGFD\"):\n",
    "    '''\n",
    "    Loads filtered data from AGFD_download.parquet file.\n",
    "\n",
    "    Parameters: \n",
    "        args (dict): Dictionary with filter conditions. Supported keys: 'class', 'rpm','severity','rpm','torque','installation','healthy_GP.\n",
    "        base_dir (str): Directory containing AGFD parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered concatenated data.\n",
    "    '''\n",
    "    if not args or not isinstance(args, dict):\n",
    "        raise ValueError(\"args must be a non-empty dictionary with filter values.\")\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "    result_dfs = []\n",
    "\n",
    "    print(base_path.glob(\"*_chunk_*\"))\n",
    "    for chunk_file in sorted(base_path.glob(\"*_chunk_*\")):\n",
    "        print(f\"Scanning {chunk_file}\")\n",
    "        try:\n",
    "            df = pd.read_parquet(chunk_file)\n",
    "            df = df.copy()  \n",
    "            for col in df.select_dtypes(include=\"object\").columns:\n",
    "                df[col] = df[col].str.strip()\n",
    "            print({col: df[col].unique() for col in args.keys() if col in df.columns}) # Check all possible parameters\n",
    "            \n",
    "            condition = pd.Series([True] * len(df))\n",
    "            \n",
    "            for col, valid_values in args.items():\n",
    "                if col in df.columns:\n",
    "                    condition &= df[col].isin(valid_values)\n",
    "                else:\n",
    "                    print(f\"Warning: Column '{col}' not found in data. Ignoring this filter.\")\n",
    "            \n",
    "            filtered = df[condition]\n",
    "            \n",
    "            if not filtered.empty:\n",
    "                result_dfs.append(filtered)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {chunk_file}: {e}\")\n",
    "\n",
    "    if result_dfs:\n",
    "        return pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "filters = {\n",
    "    \"class\": [\"healthy\"],\n",
    "    \"rpm\": [1000],\n",
    "    \"healthy_GP\":[9],\n",
    "    \"torque\":[6]\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "df = load_AGFD(filters)\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWRU\n",
    "\n",
    "```\n",
    "measurement location fault location fault type  fault depth  \\\n",
    "0                   DE             DE         OR           14   \n",
    "1                   FE             DE         OR           14   \n",
    "2                   BA             DE         OR           14   \n",
    "3                   DE             FE          B           14   \n",
    "4                   FE             FE          B           14   \n",
    "\n",
    "  fault orientation  sampling rate  torque tags  \\\n",
    "0                 C             48       1   []   \n",
    "1                 C             48       1   []   \n",
    "2                 C             48       1   []   \n",
    "3                 -             12       1   []   \n",
    "4                 -             12       1   []   \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data/CWRU/CWRU_downloaded.parquet\n",
      "{'fault type': <StringArray>\n",
      "['OR', 'B', 'IR', 'normal']\n",
      "Length: 4, dtype: string, 'fault depth': array([14,  7, 21, 28,  0]), 'torque': array([1, 0, 3, 2])}\n",
      "(1698878, 10)\n",
      "         measurement_id  sample_index  measurement measurement location  \\\n",
      "0  48k_DE_OR-C_014_1_DE             0     0.133305                   DE   \n",
      "1  48k_DE_OR-C_014_1_DE             1     0.152498                   DE   \n",
      "2  48k_DE_OR-C_014_1_DE             2     0.166475                   DE   \n",
      "\n",
      "  fault location fault type  fault depth fault orientation  sampling rate  \\\n",
      "0             DE         OR           14                 C             48   \n",
      "1             DE         OR           14                 C             48   \n",
      "2             DE         OR           14                 C             48   \n",
      "\n",
      "   torque  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_CWRU(args: dict, base_dir=\"data/CWRU\"):\n",
    "    '''\n",
    "    Loads filtered data from AGFD_download.parquet file.\n",
    "\n",
    "    Parameters: \n",
    "        args (dict): Dictionary with filter conditions. Supported keys: 'class', 'rpm','severity','rpm','torque','installation','healthy_GP.\n",
    "        base_dir (str): Directory containing AGFD parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered concatenated data.\n",
    "    '''\n",
    "    if not args or not isinstance(args, dict):\n",
    "        raise ValueError(\"args must be a non-empty dictionary with filter values.\")\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "    result_dfs = []\n",
    "\n",
    "    file = next(base_path.glob(\"*.parquet\"), None)\n",
    "\n",
    "    if file is None:\n",
    "        raise FileNotFoundError(f\"No .parquet file found in {base_dir}\")\n",
    "    print(f\"Scanning {file}\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file)\n",
    "        print({col: df[col].unique() for col in args.keys() if col in df.columns})\n",
    "        \n",
    "        for col in df.select_dtypes(include=\"object\").columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "        condition = pd.Series([True] * len(df))\n",
    "        for col, values in args.items():\n",
    "            if col in df.columns:\n",
    "                condition &= df[col].isin(values)\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in chunk, skipping filter.\")\n",
    "\n",
    "        filtered = df[condition]\n",
    "        if not filtered.empty:\n",
    "            result_dfs.append(filtered)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {file}: {e}\")\n",
    "\n",
    "    if result_dfs:\n",
    "        return pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "filters = {\n",
    "    \"fault type\": [\"OR\"],\n",
    "    \"fault depth\":[14],\n",
    "    \"torque\":[1]\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "df = load_CWRU(filters)\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
