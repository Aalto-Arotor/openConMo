{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to download AGFD and transform into a single feather file\n",
    "\n",
    "Currently requires manual extraction of .rar files.\\\n",
    "Current df structure: \\\n",
    "Sensors, class, rpm \\\n",
    "(number,thickness (mm),class number):\\\n",
    "``````\n",
    "fault_map = {\n",
    "    \"Mild_Pitting_GP1\": (\"mild\", \"pitting\"),\n",
    "    \"Severe_Pitting_GP6\": (\"severe\", \"pitting\"),\n",
    "    \"Mild_Wear_GP7\": (\"mild\", \"wear\"),\n",
    "    \"Severe_Wear_GP2\": (\"severe\", \"wear\"),\n",
    "    \"Mild_Micropitting_GP4\": (\"mild\", \"micropitting\"),\n",
    "    \"Severe_Micropitting_GP3\": (\"severe\", \"micropitting\"),\n",
    "    \"Mild_TFF_GP9\": (\"mild\", \"crack\"),\n",
    "    \"Severe_TFF_GP5\": (\"severe\", \"crack\"),\n",
    "}\\\n",
    "``````\n",
    "Size (.rar): 10GB\\\n",
    "HOW TO USE: Run to download .rar file. Extract the data from .rar file into the \"RAW\" folder. Run the same script again to obtain a pandas dataframe saved into a .feather file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    " \n",
    "\n",
    "def download(url: str, filepath: str, chunk_size=1024):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    block_size = 1024\n",
    " \n",
    "    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    " \n",
    " \n",
    "# FILE CHECK / DOWNLOAD\n",
    "#######################\n",
    " \n",
    "#cur_dir = Path(__file__).parent.parent # IF from .py file\n",
    "try:\n",
    "    # Works in .py script\n",
    "        cur_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback for notebooks where __file__ is undefined\n",
    "        cur_dir = Path(os.getcwd())\n",
    "\n",
    "download_dir = cur_dir / \"data\" / \"AGFD\" / \"RAW\"\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "conversion = True # TOGGLE TRUE FOR 32 float conversion (True if data used for DL, false if 64 floats required)\n",
    " \n",
    "if (download_dir / \"3012Hz\").is_dir():\n",
    "    # All ok\n",
    "    pass\n",
    "elif (download_dir / \"AGFD.rar\").is_file():\n",
    "    print(\n",
    "        \"Please extract the AGFD.rar file contents to the data/AGFD/RAW directory and run the script again.\"\n",
    "    )\n",
    "    quit()\n",
    "else:\n",
    "    print(\"Downloading AGFD dataset...\")\n",
    "    print()\n",
    " \n",
    "    # Mendeley data download url\n",
    "    # For manual download, visit: https://data.mendeley.com/datasets/fywnj597d8/2\n",
    "    url = \"https://data.mendeley.com/public-files/datasets/fywnj597d8/files/1888dd1e-4ceb-401b-800a-fb49443a1cbc/file_downloaded\"\n",
    " \n",
    "    download(url, download_dir / \"AGFD.rar\")\n",
    " \n",
    "    print()\n",
    "    print(\"Download complete.\")\n",
    "    print(\n",
    "        \"Please extract the AGFD.rar file contents to the data/AGFD/RAW directory and run the script again.\"\n",
    "    )\n",
    " \n",
    "# CLEANING\n",
    "##########\n",
    " \n",
    "# HELPERS\n",
    " \n",
    " \n",
    "def fix_time(X):\n",
    "    increment = 1 / 3012  # For 3012 Hz sampling rate\n",
    "    loc = np.where(X[1:] < X[:-1])[0] + 1  # + 1 needed to get the right index\n",
    " \n",
    "    if len(loc) == 0:\n",
    "        return X\n",
    " \n",
    "    for l in loc:\n",
    "        wrong = X[l - 1] - X[l]\n",
    "        print(f\"Wrong time at {l}\")\n",
    "        diff = np.concatenate(\n",
    "            [\n",
    "                np.zeros(l),\n",
    "                np.ones(len(X) - l) * (increment + wrong),\n",
    "            ]\n",
    "        )\n",
    "        X = X + diff\n",
    " \n",
    "    return X\n",
    " \n",
    " \n",
    "# PROCESSING\n",
    " \n",
    "torque_map = {\n",
    "    \"0_12Nm\": 1,\n",
    "    \"0_71Nm\": 6,\n",
    "    \"1_31Nm\": 11,\n",
    "}\n",
    " \n",
    "fault_map = {\n",
    "    \"Mild_Pitting_GP1\": (\"mild\", \"pitting\"),\n",
    "    \"Severe_Pitting_GP6\": (\"severe\", \"pitting\"),\n",
    "    \"Mild_Wear_GP7\": (\"mild\", \"wear\"),\n",
    "    \"Severe_Wear_GP2\": (\"severe\", \"wear\"),\n",
    "    \"Mild_Micropitting_GP4\": (\"mild\", \"micropitting\"),\n",
    "    \"Severe_Micropitting_GP3\": (\"severe\", \"micropitting\"),\n",
    "    \"Mild_TFF_GP9\": (\"mild\", \"crack\"),\n",
    "    \"Severe_TFF_GP5\": (\"severe\", \"crack\"),\n",
    "}\n",
    " \n",
    "# Specify raw data\n",
    "files = download_dir.glob(\"**/*.csv\")\n",
    " \n",
    "# Go through files\n",
    "dfs = []\n",
    "for f in files:\n",
    "    # SPECS\n",
    "    ##\n",
    " \n",
    "    # Get measurement specifications from file path\n",
    "    p = f.parts\n",
    " \n",
    "    if p[-2] == \"Healthy\":\n",
    "        rpm = int(p[-4].replace(\"RPM\", \"\"))\n",
    "        torque = torque_map[p[-3]]\n",
    "        installation = 0\n",
    "        severity = \"-\"\n",
    "        GP = int(p[-1][2])\n",
    "        fault = \"healthy\"\n",
    "    elif p[-3] == \"Faulty\":\n",
    "        rpm = int(p[-5].replace(\"RPM\", \"\"))\n",
    "        torque = torque_map[p[-4]]\n",
    "        installation = int(p[-2][-1])\n",
    "        severity, fault = fault_map.get(\n",
    "            p[-1].replace(\".csv\", \"\"), (None, None)\n",
    "        )  # Fault types we don't care about not in map\n",
    "        GP = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type directory\")\n",
    " \n",
    "    # Skip unknown faults\n",
    "    if fault is None:\n",
    "        raise ValueError(f\"Unknown fault type: {p[-1]}\")\n",
    " \n",
    "    # SIGNAL\n",
    "    ##\n",
    " \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(f, sep=\",\", index_col=0, header=0)\n",
    " \n",
    "    print(str(f))\n",
    " \n",
    "    # Add info from file name\n",
    "    df[\"class\"] = fault\n",
    "    df[\"rpm\"] = rpm\n",
    "    df[\"torque\"] = torque\n",
    "    df[\"installation\"] = installation\n",
    "    df[\"severity\"] = severity\n",
    "    df[\"healthy_GP\"] = GP\n",
    " \n",
    "    # Fix signal data\n",
    "    #################\n",
    " \n",
    "    # Enc4 runs the wrong way in most files\n",
    "    if df[\"enc4_ang\"].iloc[0] > df[\"enc4_ang\"].iloc[1]:\n",
    "        df[\"enc4_ang\"] = -df[\"enc4_ang\"]\n",
    " \n",
    "    # Some measurements have glitches in time\n",
    "    df[\"time\"] = fix_time(df[\"time\"].to_numpy())\n",
    " \n",
    " \n",
    "    # Make DF of one file\n",
    "    dfs.append(df)\n",
    " \n",
    "\n",
    "# Combine files \n",
    "dfs = pd.concat(dfs)\n",
    " \n",
    "# * Conversion done because deep learning computations are done with float32 anyway \n",
    "# Get float 64 columns\n",
    "if conversion:\n",
    "\n",
    "    float64_cols = list(dfs.select_dtypes(include=\"float64\"))\n",
    "    # Convert those columns float 32 pitäiskö tehdä?\n",
    "    dfs[float64_cols] = dfs[float64_cols].astype(\"float32\")\n",
    " \n",
    "string_cols = [\n",
    "    \"class\",\n",
    "    \"severity\",\n",
    "]\n",
    "dfs[string_cols] = dfs[string_cols].astype(\"string\")\n",
    " \n",
    "# Reset index to counteract concatenating a bunch of separate dataframes\n",
    "dfs = dfs.reset_index(drop=True)\n",
    " \n",
    "print(\"Converssion to feather done.\")\n",
    "print(\"Saving to feather...\")\n",
    " \n",
    "dfs.to_feather(download_dir.parent / \"AGFD_downloaded.feather\")\n",
    " \n",
    "print(\"Saving done.\")\n",
    "print()\n",
    " \n",
    "print(\"Dataframe shape:\", dfs.shape)\n",
    "print(\"Data types:\")\n",
    "print(dfs.dtypes)\n",
    " \n",
    "print()\n",
    "print(\"First 3 rows of the dataframe:\")\n",
    "print(dfs.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs['severity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
