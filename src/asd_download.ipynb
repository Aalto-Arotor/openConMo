{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to download ASD and transform into a single feather file \n",
    "\n",
    "Currently requires manual extraction of .rar files.\\\n",
    "Current df structure: \\\n",
    "Sensors, class, rpm \\\n",
    "Induced faults by gluing varying numbers of shim metal sheets between meshing gear teeth.  \n",
    "(number,thickness (mm),class number):\\\n",
    "``````\n",
    "fault_map = {\n",
    "    \"Healthy\": (\"0\", \"0\",\"0\"),\n",
    "    \"Failure1\": (\"1\", \"0.01\", \"1\"),\n",
    "    \"Failure2\": (\"2\", \"0.01\", \"2\"),\n",
    "    \"Failure3\": (\"3\", \"0.01\", \"3\"),\n",
    "    \"Failure4\": (\"1\", \"0.03\", \"4\"),\n",
    "    \"Failure5\": (\"2\", \"0.03\", \"5\"),\n",
    "    \"Failure6\": (\"3\", \"0.03\", \"6\"),\n",
    "    \"Failure7\": (\"1\", \"0.05\", \"7\"),\n",
    "    \"Failure8\": (\"2\", \"0.05\", \"8\"),\n",
    "    \"Failure9\": (\"3\", \"0.05\", \"9\")\n",
    "}\n",
    "``````\n",
    "\\\n",
    "Size (.rar): 500Mb\n",
    "HOW TO USE: Run to download .rar file. Extract the data from .rar file into the \"RAW\" folder. Run the same script again to obtain a pandas dataframe saved into a .feather file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    " \n",
    "\n",
    "def download(url: str, filepath: str, chunk_size=1024):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    block_size = 1024\n",
    " \n",
    "    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    " \n",
    " \n",
    "# FILE CHECK / DOWNLOAD\n",
    "#######################\n",
    " \n",
    "#cur_dir = Path(__file__).parent.parent # IF from .py file\n",
    "try:\n",
    "    # Works in .py script\n",
    "        cur_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback for notebooks where __file__ is undefined\n",
    "        cur_dir = Path(os.getcwd())\n",
    "\n",
    "download_dir = cur_dir / \"data\" / \"ASD\" / \"RAW\"\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "conversion = True # TOGGLE TRUE FOR 32 float conversion (True if data used for DL, false if 64 floats required)\n",
    " \n",
    "if (download_dir / \"ASD\").is_dir():\n",
    "    # All ok\n",
    "    pass\n",
    "elif (download_dir / \"ASD.rar\").is_file():\n",
    "    print(\n",
    "        \"Please extract the ASD.rar file contents to the data/ASD/RAW directory and run the script again.\"\n",
    "    )\n",
    "    quit()\n",
    "else:\n",
    "    print(\"Downloading ASD dataset...\")\n",
    "    print()\n",
    " \n",
    "    # Mendeley data download url\n",
    "    # For manual download, visit: https://data.mendeley.com/datasets/fsjhhrw2y8/1\n",
    "    url = \"https://data.mendeley.com/public-files/datasets/fsjhhrw2y8/files/82cee859-8e9f-4cb0-9337-850679ea0e86/file_downloaded\"\n",
    " \n",
    "    download(url, download_dir / \"ASD.rar\")\n",
    " \n",
    "    print()\n",
    "    print(\"Download complete.\")\n",
    "    print(\n",
    "        \"Please extract the ASD.rar file contents to the data/ASD/RAW directory and run the script again.\" # This needs to be changed, can rar files be extracted with python?\n",
    "    )\n",
    "    # info for .rar extraction on mac: https://discussions.apple.com/thread/255141368?sortBy=rank\n",
    " \n",
    "\n",
    " \n",
    "# ASD dataset fault map\n",
    "    #(number, thickness , class number)\n",
    "fault_map = {\n",
    "    \"Healthy\": (\"0\", \"0\",\"0\"),\n",
    "    \"Failure1\": (\"1\", \"0.01\", \"1\"),\n",
    "    \"Failure2\": (\"2\", \"0.01\", \"2\"),\n",
    "    \"Failure3\": (\"3\", \"0.01\", \"3\"),\n",
    "    \"Failure4\": (\"1\", \"0.03\", \"4\"),\n",
    "    \"Failure5\": (\"2\", \"0.03\", \"5\"),\n",
    "    \"Failure6\": (\"3\", \"0.03\", \"6\"),\n",
    "    \"Failure7\": (\"1\", \"0.05\", \"7\"),\n",
    "    \"Failure8\": (\"2\", \"0.05\", \"8\"),\n",
    "    \"Failure9\": (\"3\", \"0.05\", \"9\")\n",
    "}\n",
    " \n",
    "# Specify raw data\n",
    "files = download_dir.glob(\"**/*.csv\")\n",
    " \n",
    "# Go through files\n",
    "dfs = []\n",
    "for f in files:\n",
    "    # SPECS\n",
    "    ##\n",
    " \n",
    "    # Get measurement specifications from file path\n",
    "    p = f.parts\n",
    "    print(p)\n",
    " \n",
    "    rpm = int(p[-2].replace(\"RPM\", \"\"))\n",
    "    number, thickness, label = fault_map.get(            # number = how many shims, thickness = the thickness of the shim\n",
    "        p[-1].replace(\".csv\", \"\"), (None, None)\n",
    "    )\n",
    "    fault = \"healthy\"\n",
    "    fault = \"shim\"\n",
    "   \n",
    " \n",
    "    # Skip unknown faults\n",
    "    if fault is None:\n",
    "        raise ValueError(f\"Unknown fault type: {p[-1]}\")\n",
    " \n",
    "    # SIGNAL\n",
    "    ##\n",
    " \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(f, sep=\",\", index_col=0, header=0)\n",
    " \n",
    "    print(str(f))\n",
    " \n",
    "    # Add info from file name\n",
    "    \n",
    "    df[\"class\"] = label\n",
    "    df[\"rpm\"] = rpm\n",
    "\n",
    " \n",
    "    # Make DF of one file\n",
    "    dfs.append(df)\n",
    "    #print(len(dfs))\n",
    " \n",
    "# dfs listassa kaikki järkevässä muodossa\n",
    "# Combine files <--- Toimii!\n",
    "dfs = pd.concat(dfs)\n",
    " \n",
    "# * Conversion done because deep learning computations are done with float32 anyway <--- kaikki ei tee välttämättä DL\n",
    "# Get float 64 columns\n",
    "if conversion:\n",
    "\n",
    "    float64_cols = list(dfs.select_dtypes(include=\"float64\"))\n",
    "    # Convert those columns float 32 pitäiskö tehdä?\n",
    "    dfs[float64_cols] = dfs[float64_cols].astype(\"float32\")\n",
    " \n",
    "string_cols = [\n",
    "    \"class\",\n",
    "    \"rpm\",\n",
    "]\n",
    "dfs[string_cols] = dfs[string_cols].astype(\"string\")\n",
    " \n",
    "# Reset index to counteract concatenating a bunch of separate dataframes\n",
    "dfs = dfs.reset_index(drop=True)\n",
    " \n",
    "print(\"Converssion to feather done.\")\n",
    "print(\"Saving to feather...\")\n",
    " \n",
    "dfs.to_feather(download_dir.parent / \"ASD_downloaded.feather\")\n",
    " \n",
    "print(\"Saving done.\")\n",
    "print()\n",
    " \n",
    "print(\"Dataframe shape:\", dfs.shape)\n",
    "print(\"Data types:\")\n",
    "print(dfs.dtypes)\n",
    " \n",
    "print()\n",
    "print(\"First 3 rows of the dataframe:\")\n",
    "print(dfs.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs['rpm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
